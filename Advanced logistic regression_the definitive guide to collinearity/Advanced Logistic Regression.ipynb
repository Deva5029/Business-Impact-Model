{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced logistic regression: the definitive guide to collinearity\n",
    "\n",
    "Among the [most popular machine learning algorithms](https://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html), [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) is definitely one you should have a handle on.  There's a lot of online courses pushing neural networks and deep learning, but chances are, you probably won't encounter either of these in interviews.\n",
    "\n",
    "Instead, you should have a strong handle on the basics.  This includes models such as logistic regression and linear regression.\n",
    "\n",
    "In this article we'll focus on logistic regression, a linear classifier.  More specifically, we'll focus on one of the major problems with linear models, collinearity.\n",
    "\n",
    "We'll walk through a number of examples, along with some Python code.  After reading through this article you'll have a better grasp on collinearity, so that you can ace your next data science interview.\n",
    "\n",
    "## Logistic Regression Overview\n",
    "To start, I'm going to give a brief review of logistic regression.  I won't get into too much detail, because I want to focus on advanced topics you might encounter in interviews.  \n",
    "\n",
    "Logistic regression is a supervised learning model.  The model predicts the probability of an outcome using the logistic function.\n",
    "\n",
    "<img src=\"log_function3.jpg\">\n",
    "\n",
    "The model \"learns\", or estimates, the parameters typically using the [maximum likelihood estimation](https://stats.stackexchange.com/questions/112451/maximum-likelihood-estimation-mle-in-layman-terms).  I'll leave these details for another post, just be aware that this is happening behind the scenes.\n",
    "\n",
    "It's typically used for classification problems with binary outcomes, but can also be used for classification problems with multiple outcomes.\n",
    "\n",
    "Here are some [typical applications](https://www.quora.com/What-are-applications-of-linear-and-logistic-regression):\n",
    "1. Customer churn \n",
    "2. Geographic image processing \n",
    "3. Handwriting recognition \n",
    "4. Healthcare analytics, such as risk of heart attack\n",
    "\n",
    "Logistic regression is [\"linear\"](https://stats.stackexchange.com/questions/93569/why-is-logistic-regression-a-linear-classifier) because the decision boundary separating the classes is linear.  \n",
    "\n",
    "<img src=\"linsep_new.png\">\n",
    "\n",
    "This is a common point of confusion, because when you look at the S shaped logistic function, it's certainly not linear.  \n",
    "\n",
    "<img src=\"log_fun_lin.jpg\">\n",
    "\n",
    "What makes logistic regression \"linear\" is the linear combination of the features. \n",
    "\n",
    "<img src=\"log_odds.jpg\">\n",
    "\n",
    "Logistic regression is a commonly used model, but it shouldn't be used for highly non-linear data.  \n",
    "\n",
    "This is the classic example of the [bias-variance tradeoff](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/).  Because logistic regression is a linear classifier, it has a high bias towards this type of fit.  Likewise, it's lack of flexibility means that it has very low-variance, so it won't be able to predict non-linear outcomes very well.\n",
    "\n",
    "What we gain in model interpretability from a simple linear model, we lose in flexibility to fit to non-linear data.  This bias-variance tradeoff is always a consideration.\n",
    "\n",
    "\n",
    "## Collinearity \n",
    "\n",
    "Lets get started with some advanced topics using a hypothetical example.\n",
    "\n",
    "You work in the risk department for a mortgage lender, and it's your job to pre-screen people for risk of default on loans.  \n",
    "\n",
    "<img src=\"accept_reject.jpg\">\n",
    "\n",
    "There's a number of factors you're looking at, including income, credit rating, and credit limit.\n",
    "\n",
    "Looking at all of these factors, you're trying to distinguish which ones are providing useful, somewhat unique, information.  You decide to take a closer look at credit score, and credit limit.  Here's what the data looks like:\n",
    "\n",
    "<img src=\"rating_limit.jpg\">\n",
    "\n",
    "Obviously this data is highly correlated.  A person with a higher credit score usually has a higher credit limit.  \n",
    "\n",
    "People with higher credit scores are probably less risky.  Likewise, people with high credit limits are probably less risky.  \n",
    "\n",
    "Both of these measures seem to be providing useful information about credit, but it's essentially the same information.  The question is, which one is really contributing information to my model?\n",
    "\n",
    "This is a classic case of collinearity.  Now let's dive deeper into some details, and see just exactly why it can be a problem.\n",
    "\n",
    "## Model interpretability\n",
    "\n",
    "Linear models are a great tool because they're easy to interpret.  If we can predict the coefficients of the features, we can interpret how a change in that feature will affect the outcome of our model.  \n",
    "\n",
    "Let's run through an example to illustrate what I mean.  I'll be using Python's [statsmodels](http://www.statsmodels.org/stable/index.html) library to fit the logistic regression models in this section.\n",
    "\n",
    "First, start by downloading the dataset, default.csv (link to github).  Now import the libraries and read in the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   income  limit  rating  default\n",
      "0   17456   5154     495        0\n",
      "1   19103   5658     527        0\n",
      "2   21531   4214     502        0\n",
      "3   21711   6341     555        0\n",
      "4   25455   4862     452        0\n",
      "     income  limit  rating  default\n",
      "342   55480    852      39        1\n",
      "343   62202   3022     335        1\n",
      "344   64063   2906     350        1\n",
      "345   64663   2844     350        1\n",
      "346   76462   4661     489        1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# Reading in and taking a look at the data\n",
    "df = pd.read_csv(\"default.csv\")\n",
    "print df.head()\n",
    "print df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to add a dummy variable of 1.0 so that a coefficient can be fit for the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   income  limit  rating  default  intercept\n",
      "0   17456   5154     495        0        1.0\n",
      "1   19103   5658     527        0        1.0\n",
      "2   21531   4214     502        0        1.0\n",
      "3   21711   6341     555        0        1.0\n",
      "4   25455   4862     452        0        1.0\n",
      "     income  limit  rating  default  intercept\n",
      "342   55480    852      39        1        1.0\n",
      "343   62202   3022     335        1        1.0\n",
      "344   64063   2906     350        1        1.0\n",
      "345   64663   2844     350        1        1.0\n",
      "346   76462   4661     489        1        1.0\n"
     ]
    }
   ],
   "source": [
    "# adding a column of ones to fit the intercept\n",
    "df['intercept'] = 1.0\n",
    "print df.head()\n",
    "print df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'm going to split my data into three folds for training and testing the model.  This is known as k-fold cross-validation, which in my case is three-fold cross-validation.  This is a useful technique because it hides the test data from the model to try and prevent overfitting.\n",
    "\n",
    "*image of 3 fold cross validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 features:\n",
      "   intercept  income  limit  rating\n",
      "0        1.0   17456   5154     495\n",
      "1        1.0   19103   5658     527\n",
      "2        1.0   21531   4214     502\n",
      "3        1.0   21711   6341     555\n",
      "4        1.0   25455   4862     452\n",
      "Fold 1 outcomes:\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: default, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# feaure variables\n",
    "features = ['intercept','income','limit','rating']\n",
    "\n",
    "# creating 3 folds\n",
    "fold1 = df[features][0:116]\n",
    "y_fold1 = df['default'][0:116]\n",
    "\n",
    "fold2 = df[features][116:231]\n",
    "y_fold2 = df['default'][116:231]\n",
    "\n",
    "fold3 = df[features][231:]\n",
    "y_fold3 = df['default'][231:]\n",
    "\n",
    "print \"Fold 1 features:\"\n",
    "print fold1.head()\n",
    "print \"Fold 1 outcomes:\"\n",
    "print y_fold1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by fitting a model to fold 1.  We'll keep all of the features to start.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.479695\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>default</td>     <th>  No. Observations:  </th>  <td>   116</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   112</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Wed, 27 Dec 2017</td> <th>  Pseudo R-squ.:     </th>  <td>0.3078</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>10:42:29</td>     <th>  Log-Likelihood:    </th> <td> -55.645</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -80.388</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>1.028e-10</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>    5.6647</td> <td>    1.202</td> <td>    4.712</td> <td> 0.000</td> <td>    3.309</td> <td>    8.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>    <td> -4.94e-05</td> <td> 1.23e-05</td> <td>   -4.007</td> <td> 0.000</td> <td>-7.36e-05</td> <td>-2.52e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>limit</th>     <td>   -0.0005</td> <td>    0.000</td> <td>   -1.746</td> <td> 0.081</td> <td>   -0.001</td> <td> 6.64e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rating</th>    <td>   -0.0026</td> <td>    0.004</td> <td>   -0.739</td> <td> 0.460</td> <td>   -0.009</td> <td>    0.004</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                default   No. Observations:                  116\n",
       "Model:                          Logit   Df Residuals:                      112\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Wed, 27 Dec 2017   Pseudo R-squ.:                  0.3078\n",
       "Time:                        10:42:29   Log-Likelihood:                -55.645\n",
       "converged:                       True   LL-Null:                       -80.388\n",
       "                                        LLR p-value:                 1.028e-10\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept      5.6647      1.202      4.712      0.000       3.309       8.021\n",
       "income      -4.94e-05   1.23e-05     -4.007      0.000   -7.36e-05   -2.52e-05\n",
       "limit         -0.0005      0.000     -1.746      0.081      -0.001    6.64e-05\n",
       "rating        -0.0026      0.004     -0.739      0.460      -0.009       0.004\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model on fold 1\n",
    "logit = sm.Logit(y_fold1, fold1)\n",
    "result = logit.fit()\n",
    "\n",
    "# summary\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at the results and see what's going on.  \n",
    "\n",
    "To assess our model, we'll use the p-value (https://en.wikipedia.org/wiki/P-value#Basic_concepts).  The p-value is essentially a threshold which we use to determine if we have reached a level of significance (http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-interpret-regression-analysis-results-p-values-and-coefficients) for which we can reject the null hypothesis.  Typically we reject the null hypothesis if p < 0.05, but we can set the threshold even tighter, to 0.01 or 0.005.\n",
    "\n",
    "In terms of our model, the null hypothesis is that the regression coefficient is equal to zero.  Basically this means that if we don't have a low p-value, we don't have strong enough statistical evidence to include that variable in our model.  \n",
    "*show an image... null hypothesis*\n",
    "*p < 0.05, reject the null... include the variable*\n",
    "\n",
    "Both the intercept and income term have a p-value below 0.05, but limit and rating do not.  In this case, we don't have strong enough statistical evidence to justify keeping limit and rating in the model at the same time.\n",
    "\n",
    "Now lets take a look at a summary of results for the seoncd fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.452194\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>default</td>     <th>  No. Observations:  </th>  <td>   115</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   111</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Wed, 27 Dec 2017</td> <th>  Pseudo R-squ.:     </th>  <td>0.3476</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>10:42:31</td>     <th>  Log-Likelihood:    </th> <td> -52.002</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -79.708</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>5.612e-12</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>    5.8389</td> <td>    1.264</td> <td>    4.621</td> <td> 0.000</td> <td>    3.362</td> <td>    8.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>    <td> -5.85e-05</td> <td> 1.42e-05</td> <td>   -4.126</td> <td> 0.000</td> <td>-8.63e-05</td> <td>-3.07e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>limit</th>     <td>-4.223e-05</td> <td>    0.000</td> <td>   -0.116</td> <td> 0.908</td> <td>   -0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rating</th>    <td>   -0.0077</td> <td>    0.005</td> <td>   -1.637</td> <td> 0.102</td> <td>   -0.017</td> <td>    0.002</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                default   No. Observations:                  115\n",
       "Model:                          Logit   Df Residuals:                      111\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Wed, 27 Dec 2017   Pseudo R-squ.:                  0.3476\n",
       "Time:                        10:42:31   Log-Likelihood:                -52.002\n",
       "converged:                       True   LL-Null:                       -79.708\n",
       "                                        LLR p-value:                 5.612e-12\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept      5.8389      1.264      4.621      0.000       3.362       8.316\n",
       "income      -5.85e-05   1.42e-05     -4.126      0.000   -8.63e-05   -3.07e-05\n",
       "limit      -4.223e-05      0.000     -0.116      0.908      -0.001       0.001\n",
       "rating        -0.0077      0.005     -1.637      0.102      -0.017       0.002\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model on the second fold\n",
    "logit = sm.Logit(y_fold2, fold2)\n",
    "result = logit.fit()\n",
    "\n",
    "# summary\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at these new results, we notice that the coefficient for limit has changed by an order of magnitude, and rating has changed by a factor of 3.  \n",
    "\n",
    "On the other hand, intercept and income which still have p-values well below 0.05, have not changed much.  Finally, lets take a look at a result of the estimates on the third fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.451895\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>default</td>     <th>  No. Observations:  </th>  <td>   116</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   112</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Wed, 27 Dec 2017</td> <th>  Pseudo R-squ.:     </th>  <td>0.3479</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>10:42:32</td>     <th>  Log-Likelihood:    </th> <td> -52.420</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -80.388</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>4.335e-12</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>    6.2255</td> <td>    1.239</td> <td>    5.024</td> <td> 0.000</td> <td>    3.797</td> <td>    8.654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>    <td>-6.343e-05</td> <td> 1.42e-05</td> <td>   -4.457</td> <td> 0.000</td> <td>-9.13e-05</td> <td>-3.55e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>limit</th>     <td>  2.61e-05</td> <td>    0.000</td> <td>    0.081</td> <td> 0.936</td> <td>   -0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rating</th>    <td>   -0.0091</td> <td>    0.004</td> <td>   -2.086</td> <td> 0.037</td> <td>   -0.018</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                default   No. Observations:                  116\n",
       "Model:                          Logit   Df Residuals:                      112\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Wed, 27 Dec 2017   Pseudo R-squ.:                  0.3479\n",
       "Time:                        10:42:32   Log-Likelihood:                -52.420\n",
       "converged:                       True   LL-Null:                       -80.388\n",
       "                                        LLR p-value:                 4.335e-12\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept      6.2255      1.239      5.024      0.000       3.797       8.654\n",
       "income     -6.343e-05   1.42e-05     -4.457      0.000   -9.13e-05   -3.55e-05\n",
       "limit        2.61e-05      0.000      0.081      0.936      -0.001       0.001\n",
       "rating        -0.0091      0.004     -2.086      0.037      -0.018      -0.001\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model on the third fold\n",
    "logit = sm.Logit(y_fold3, fold3)\n",
    "result = logit.fit()\n",
    "\n",
    "# summary\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a summary of the coefficient estimates on all three folds:\n",
    "\n",
    "| fold | income     | limit      | rating  |\n",
    "|------|------------|------------|---------|\n",
    "| 1    | -0.0000494 | -0.0005    | -0.0026 |\n",
    "| 2    | -0.0000585 | -0.0000422 | -0.0077 |\n",
    "| 3    | -0.0000634 | 0.0000261  | -0.0091 |\n",
    "\n",
    "Based on this experiment, we can see that there's considerable instability in the coefficient estimates of limit and rating.  The numbers are jumping around all over the place.  If you were trying to interpret how a change in limit or rating affected your outcome, it would be nearly impossible.\n",
    "\n",
    "Income on the other hand, is fairly stable.  The value changes slightly from fold to fold, but not nearly as drastically as limit and rating.  This feature is much more interpretable.\n",
    "\n",
    "This example illustrates the issue of collinearity.  When two collinear variables are included in a model, it's very difficult to interpret either ones affect on the outcome of the model.\n",
    "\n",
    "In the next section, we'll look into detecting collinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting collinearity\n",
    "Now that we know that collinearity can cause issues with the coefficient estimates, lets take a look at two common techniques for [detecting collinearity](https://stats.idre.ucla.edu/stata/webbooks/logistic/chapter3/lesson-3-logistic-regression-diagnostics/).\n",
    "\n",
    "### Correlation\n",
    "One way to potentially detect collinearity is with correlation.  Lets take a look at the correlation matrix of the features from the example in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.18344745  0.24482012]\n",
      " [ 0.18344745  1.          0.84179196]\n",
      " [ 0.24482012  0.84179196  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# correlation\n",
    "print np.corrcoef([df['income'],df['limit'],df['rating']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*picture of the correlation matrix*\n",
    "We can see that our collinear variables, rating and limit, have a correlation of 0.84.  Typically anything above 0.7 is considered high.\n",
    "\n",
    "On the other hand, income, which is not collinear with either of the other variables, does not have a high correlation.  \n",
    "\n",
    "If variables have a high correlation, there is likely collinearity.  Unfortunately you can also have collinear variables that do not have a high correlation, so correlation alone should not be the only method for detecting collinearity.\n",
    "\n",
    "### Variance inflation factor\n",
    "Another more reliable way to detect collinearity is the variance inflation factor (VIF) https://en.wikipedia.org/wiki/Variance_inflation_factor.   \n",
    "\n",
    "If there's no collinearity at all, the VIF is equal to 1.  There isn't any hard and fast rule for VIF, but typically a value above 5 or 10 indicates pretty strong collinearity.  Let's take a look at the VIFs for our model.  We'll need to add in the intercept term since Python doesn't do this by default.  We'll also drop the default variable since this is our original categorical variable that we're trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income:  1.06575266304 \n",
      " limit:  3.43830427058 \n",
      " rating: 3.53443837822\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Reading in the data\n",
    "df = pd.read_csv(\"default.csv\")\n",
    "\n",
    "# adding a column of ones to fit the intercept, and dropping the 'default' feature\n",
    "df['intercept'] = 1.0\n",
    "df.pop('default')\n",
    "df.head()\n",
    "\n",
    "# changing to ndarray type for statsmodels library\n",
    "df = df.values\n",
    "\n",
    "# variance inflation factors of the features\n",
    "# don't need to include the intercept\n",
    "vif = [variance_inflation_factor(df, i) for i in range(df.shape[1])] # vif \n",
    "print \"income:  %s \\n limit:  %s \\n rating: %s\" %(vif[0],vif[1], vif[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Income is close to one, so there isn't strong colliearity with the other features.  Limit and rating are around 3, so there is probably some mild collinearity.  \n",
    "\n",
    "To show strong collinearity, lets create a new variable called income_squared, where we'll square the income variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "      <th>limit</th>\n",
       "      <th>rating</th>\n",
       "      <th>intercept</th>\n",
       "      <th>income_squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17456</td>\n",
       "      <td>5154</td>\n",
       "      <td>495</td>\n",
       "      <td>1.0</td>\n",
       "      <td>304711936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19103</td>\n",
       "      <td>5658</td>\n",
       "      <td>527</td>\n",
       "      <td>1.0</td>\n",
       "      <td>364924609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21531</td>\n",
       "      <td>4214</td>\n",
       "      <td>502</td>\n",
       "      <td>1.0</td>\n",
       "      <td>463583961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21711</td>\n",
       "      <td>6341</td>\n",
       "      <td>555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>471367521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25455</td>\n",
       "      <td>4862</td>\n",
       "      <td>452</td>\n",
       "      <td>1.0</td>\n",
       "      <td>647957025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   income  limit  rating  intercept  income_squared\n",
       "0   17456   5154     495        1.0       304711936\n",
       "1   19103   5658     527        1.0       364924609\n",
       "2   21531   4214     502        1.0       463583961\n",
       "3   21711   6341     555        1.0       471367521\n",
       "4   25455   4862     452        1.0       647957025"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the data\n",
    "df = pd.read_csv(\"default.csv\")\n",
    "\n",
    "# adding a column of ones to fit the intercept, and dropping the 'default' feature\n",
    "df['intercept'] = 1.0\n",
    "df.pop('default')\n",
    "\n",
    "# creating income_squared variable\n",
    "df['income_squared'] = df['income']**2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at the variance inflation factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income:  12.3889199368 \n",
      " limit:  3.46314456614 \n",
      " rating: 3.54893993398 \n",
      " income_squared: 12.3267258883\n"
     ]
    }
   ],
   "source": [
    "# changing to ndarray type for statsmodels library\n",
    "df = df.values\n",
    "\n",
    "# variance inflation factors of the features\n",
    "# don't need to include the intercept\n",
    "vif = [variance_inflation_factor(df, i) for i in range(df.shape[1])] # vif \n",
    "print \"income:  %s \\n limit:  %s \\n rating: %s \\n income_squared: %s\" %(vif[0],vif[1], vif[2], vif[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VIF for income has jumped from around 1 to 12.  The new variable income_squared also has a VIF around 12.  Since this new variable is based on the original variable we would expect there to be strong collinearity.\n",
    "\n",
    "Sometimes we'll want to transform a variable by squaring it, or using some other function.  It's good practice to keep the original variable as well, so there will likely be collinearity.  This isn't always a bad thing though, as we'll see later on.\n",
    "\n",
    "In this final example, we'll take a look at perfect colliearity.  Let's create another variable by adding limit and rating to create a new variable, limit_rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "      <th>limit</th>\n",
       "      <th>rating</th>\n",
       "      <th>intercept</th>\n",
       "      <th>limit_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17456</td>\n",
       "      <td>5154</td>\n",
       "      <td>495</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19103</td>\n",
       "      <td>5658</td>\n",
       "      <td>527</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21531</td>\n",
       "      <td>4214</td>\n",
       "      <td>502</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21711</td>\n",
       "      <td>6341</td>\n",
       "      <td>555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25455</td>\n",
       "      <td>4862</td>\n",
       "      <td>452</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   income  limit  rating  intercept  limit_rating\n",
       "0   17456   5154     495        1.0          5649\n",
       "1   19103   5658     527        1.0          6185\n",
       "2   21531   4214     502        1.0          4716\n",
       "3   21711   6341     555        1.0          6896\n",
       "4   25455   4862     452        1.0          5314"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the data\n",
    "df = pd.read_csv(\"default.csv\")\n",
    "\n",
    "# adding a column of ones to fit the intercept, and dropping the 'default' feature\n",
    "df['intercept'] = 1.0\n",
    "df.pop('default')\n",
    "\n",
    "# creating income_squared variable\n",
    "df['limit_rating'] = df['limit'] + df['rating']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income:  1.06575266304 \n",
      " limit:  inf \n",
      " rating: inf \n",
      " limit_rating: inf\n"
     ]
    }
   ],
   "source": [
    "# changing to ndarray type for statsmodels library\n",
    "df = df.values\n",
    "\n",
    "# variance inflation factors of the features\n",
    "# don't need to include the intercept\n",
    "vif = [variance_inflation_factor(df, i) for i in range(df.shape[1])] # vif \n",
    "print \"income:  %s \\n limit:  %s \\n rating: %s \\n limit_rating: %s\" %(vif[0],vif[1], vif[2], vif[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result for limit, rating, and limit_rating is `inf`.  Since the model couldn't compute a VIF, there must be perfect collinearity.  This is typical for a transform that is a sum of two other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources of collinearity\n",
    "\n",
    "Up to this point, we've seen a couple of different [sources of collinearity](https://stats.stackexchange.com/questions/221902/what-is-an-example-of-perfect-multicollinearity).  The first was a squared transform of the `income` variable.  Another example was the summation of the `limit` and `rating` features to create a new feature `limit_rating`.\n",
    "\n",
    "There's quite a few different examples of collinearity, but here's a short list of some of the more common ones.\n",
    "\n",
    "1. Multiple of another variable (x2 = 2x1)\n",
    "2. Add a constant to another variable (x2 = x1 + 100)\n",
    "3. Transformation of another variable (sqrt, ^2, log)\n",
    "4. \"Dummy Variable trap\" red, blue, green (3 instead of n-1, which should be 2)\n",
    "5. Multicollinearity (x1 and x2 are collinear, x3 = x2 + x1)\n",
    "\n",
    "We've already seen that collinearity can make it difficult to interpret a model, but sometimes collinearity is unavoidable.  \n",
    "\n",
    "When we transform a variable, we should keep the original variable in our model.  This is always going to cause some level of collinearity, but this might not be an issue.\n",
    "\n",
    "In the next section we'll look at several ways to deal with collinearity.\n",
    "\n",
    "## Dealing with collinearity\n",
    "If our model has collinear variables, we have several options:\n",
    "\n",
    "1. Remove collinear variables\n",
    "2. Center or standardize collinear variables\n",
    "3. Ridge regression\n",
    "4. Do nothing (more on this later...)\n",
    "\n",
    "Lets take a look at the first three in more detail.  I'll leave option four for discussion later on.\n",
    "\n",
    "### Remove collinear variables \n",
    "Going back to our original example, lets take a look at what happens when we remove collinear variables.  \n",
    "\n",
    "Lets start by reading in the data again, and slicing it into three folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "df = pd.read_csv(\"default.csv\")\n",
    "\n",
    "# adding a column of ones to fit the intercept\n",
    "df['intercept'] = 1.0\n",
    "\n",
    "# feaure variables\n",
    "features = ['intercept','income','limit','rating']\n",
    "\n",
    "# creating 3 folds\n",
    "fold1 = df[features][0:116]\n",
    "y_fold1 = df['default'][0:116]\n",
    "\n",
    "fold2 = df[features][116:231]\n",
    "y_fold2 = df['default'][116:231]\n",
    "\n",
    "fold3 = df[features][231:]\n",
    "y_fold3 = df['default'][231:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take another quick look at a summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.479695\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>default</td>     <th>  No. Observations:  </th>  <td>   116</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   112</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Wed, 27 Dec 2017</td> <th>  Pseudo R-squ.:     </th>  <td>0.3078</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>10:43:07</td>     <th>  Log-Likelihood:    </th> <td> -55.645</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -80.388</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>1.028e-10</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>    5.6647</td> <td>    1.202</td> <td>    4.712</td> <td> 0.000</td> <td>    3.309</td> <td>    8.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>    <td> -4.94e-05</td> <td> 1.23e-05</td> <td>   -4.007</td> <td> 0.000</td> <td>-7.36e-05</td> <td>-2.52e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>limit</th>     <td>   -0.0005</td> <td>    0.000</td> <td>   -1.746</td> <td> 0.081</td> <td>   -0.001</td> <td> 6.64e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rating</th>    <td>   -0.0026</td> <td>    0.004</td> <td>   -0.739</td> <td> 0.460</td> <td>   -0.009</td> <td>    0.004</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                default   No. Observations:                  116\n",
       "Model:                          Logit   Df Residuals:                      112\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Wed, 27 Dec 2017   Pseudo R-squ.:                  0.3078\n",
       "Time:                        10:43:07   Log-Likelihood:                -55.645\n",
       "converged:                       True   LL-Null:                       -80.388\n",
       "                                        LLR p-value:                 1.028e-10\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept      5.6647      1.202      4.712      0.000       3.309       8.021\n",
       "income      -4.94e-05   1.23e-05     -4.007      0.000   -7.36e-05   -2.52e-05\n",
       "limit         -0.0005      0.000     -1.746      0.081      -0.001    6.64e-05\n",
       "rating        -0.0026      0.004     -0.739      0.460      -0.009       0.004\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model on fold 1\n",
    "logit = sm.Logit(y_fold1, fold1)\n",
    "result = logit.fit()\n",
    "\n",
    "# summary\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results we see that the p-value for both `limit` and `rating` are above our threshold of 0.05.  Lets take a look at what happens when we remove `rating` from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.482046\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>default</td>     <th>  No. Observations:  </th>  <td>   116</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   113</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Wed, 27 Dec 2017</td> <th>  Pseudo R-squ.:     </th>  <td>0.3044</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>10:43:22</td>     <th>  Log-Likelihood:    </th> <td> -55.917</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -80.388</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>2.358e-11</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>    5.3920</td> <td>    1.122</td> <td>    4.805</td> <td> 0.000</td> <td>    3.193</td> <td>    7.591</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>    <td>-5.002e-05</td> <td> 1.23e-05</td> <td>   -4.077</td> <td> 0.000</td> <td>-7.41e-05</td> <td> -2.6e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>limit</th>     <td>   -0.0007</td> <td>    0.000</td> <td>   -3.627</td> <td> 0.000</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                default   No. Observations:                  116\n",
       "Model:                          Logit   Df Residuals:                      113\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Wed, 27 Dec 2017   Pseudo R-squ.:                  0.3044\n",
       "Time:                        10:43:22   Log-Likelihood:                -55.917\n",
       "converged:                       True   LL-Null:                       -80.388\n",
       "                                        LLR p-value:                 2.358e-11\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept      5.3920      1.122      4.805      0.000       3.193       7.591\n",
       "income     -5.002e-05   1.23e-05     -4.077      0.000   -7.41e-05    -2.6e-05\n",
       "limit         -0.0007      0.000     -3.627      0.000      -0.001      -0.000\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feaure variables\n",
    "features = ['intercept','income','limit']\n",
    "\n",
    "# creating 3 folds\n",
    "fold1 = df[features][0:116]\n",
    "y_fold1 = df['default'][0:116]\n",
    "\n",
    "fold2 = df[features][116:231]\n",
    "y_fold2 = df['default'][116:231]\n",
    "\n",
    "fold3 = df[features][231:]\n",
    "y_fold3 = df['default'][231:]\n",
    "\n",
    "\n",
    "# fit the model on fold 1\n",
    "logit = sm.Logit(y_fold1, fold1)\n",
    "result = logit.fit()\n",
    "\n",
    "# summary\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `limit` feature now has a p-value well below 0.05.  This means that we can safely include this variable in the model, and we can expect far less instability of the coefficient estimate.  \n",
    "\n",
    "If we include `rating` and remove `limit` we achieve a similar result.  This shows that removing a collinear variable will improve the stability of the coefficient estimates.\n",
    "\n",
    "### Center the collinear variables\n",
    "Another method to remove collinearity is to center the collinear variables.  This is done by subtracting the mean from a feature before performing a transform.  I'll walk us through another example.\n",
    "\n",
    "Lets take a look at our previous example where we created the `income_squared` feature.  Again, we'll read in the data, and create the new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>income</th>\n",
       "      <th>limit</th>\n",
       "      <th>rating</th>\n",
       "      <th>intercept</th>\n",
       "      <th>income_squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17456</td>\n",
       "      <td>5154</td>\n",
       "      <td>495</td>\n",
       "      <td>1.0</td>\n",
       "      <td>304711936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19103</td>\n",
       "      <td>5658</td>\n",
       "      <td>527</td>\n",
       "      <td>1.0</td>\n",
       "      <td>364924609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21531</td>\n",
       "      <td>4214</td>\n",
       "      <td>502</td>\n",
       "      <td>1.0</td>\n",
       "      <td>463583961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21711</td>\n",
       "      <td>6341</td>\n",
       "      <td>555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>471367521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25455</td>\n",
       "      <td>4862</td>\n",
       "      <td>452</td>\n",
       "      <td>1.0</td>\n",
       "      <td>647957025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   income  limit  rating  intercept  income_squared\n",
       "0   17456   5154     495        1.0       304711936\n",
       "1   19103   5658     527        1.0       364924609\n",
       "2   21531   4214     502        1.0       463583961\n",
       "3   21711   6341     555        1.0       471367521\n",
       "4   25455   4862     452        1.0       647957025"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the data\n",
    "df = pd.read_csv(\"default.csv\")\n",
    "\n",
    "# adding a column of ones to fit the intercept, and dropping the 'default' feature\n",
    "df['intercept'] = 1.0\n",
    "df.pop('default')\n",
    "\n",
    "# creating income_squared variable\n",
    "df['income_squared'] = df['income']**2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking another look at the VIFs, we can see that `income` and `income_squared` are collinear (VIF>10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income:  12.3889199368 \n",
      " limit:  3.46314456614 \n",
      " rating: 3.54893993398 \n",
      " income_squared: 12.3267258883\n"
     ]
    }
   ],
   "source": [
    "# changing to ndarray type for statsmodels library\n",
    "df = df.values\n",
    "\n",
    "# variance inflation factors of the features\n",
    "# don't need to include the intercept\n",
    "vif = [variance_inflation_factor(df, i) for i in range(df.shape[1])] # vif \n",
    "print \"income:  %s \\n limit:  %s \\n rating: %s \\n income_squared: %s\" %(vif[0],vif[1], vif[2], vif[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time I'll center the `income` feature before creating `income_squared`.  Lets take another look at the VIFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income:  2.75699200552 \n",
      " limit:  3.46314456614 \n",
      " rating: 3.54893993398 \n",
      " income_squared: 2.69714751516\n"
     ]
    }
   ],
   "source": [
    "# Reading in the data\n",
    "df = pd.read_csv(\"default.csv\")\n",
    "\n",
    "# removing default\n",
    "df['intercept'] = 1.0\n",
    "df.pop('default')\n",
    "\n",
    "# centering the income variable \n",
    "df['income'] = df['income'] - np.mean(df['income'])\n",
    "\n",
    "# creating income_squared variable\n",
    "df['income_squared'] = df['income']**2\n",
    "\n",
    "# changing to ndarray type for statsmodels library\n",
    "df = df.values\n",
    "\n",
    "# variance inflation factors of the features\n",
    "# don't need to include the intercept\n",
    "vif = [variance_inflation_factor(df, i) for i in range(df.shape[1])] # vif \n",
    "print \"income:  %s \\n limit:  %s \\n rating: %s \\n income_squared: %s\" %(vif[0],vif[1], vif[2], vif[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centering the `income` variable before the transform has reduced the collinearity to an acceptable level.  If we were to fit models using these features, the coefficient estimates for `income` and `income_squared` would be much more stable.\n",
    "\n",
    "Another option similar to centering is [standardizing](https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia).  In addition to subtracting the mean, we would also divide by the standard deviation.  This is another option if we want to reduce the VIF of transformed variables.\n",
    "\n",
    "### Ridge regression \n",
    "\n",
    "Another way to deal with collinearity is with a technique called Ridge Regression.  This technique uses a penalty term for collinear variables that essentially shrinks the coefficient estimates towards zero.  \n",
    "\n",
    "This shrinkage penalty minimizes the effects of the collinear variables.  Lets take a look at an example.  \n",
    "\n",
    "This time we'll use the [sklearn implementation of Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).  This uses Ridge Regression by default (penalty: 'l2').\n",
    "\n",
    "I'll use the perfectly collinear variable `limit_rating` which is a sum of the `limit` and `rating` variables.  To turn off the Ridge Regression, change the parameter `C` to a large number, such as `1e9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0010443457910360868"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Reading in the data\n",
    "df = pd.read_csv(\"default.csv\")\n",
    "\n",
    "# adding a column of ones to fit the intercept\n",
    "df['intercept'] = 1.0\n",
    "\n",
    "# creating a collinear variable limit_rating\n",
    "df['limit_rating'] = df['limit'] + df['rating']\n",
    "\n",
    "# feaure variables\n",
    "features = ['intercept','income','limit','rating','limit_rating']\n",
    "\n",
    "# creating 3 folds\n",
    "fold1 = df[features][0:116]\n",
    "y_fold1 = df['default'][0:116]\n",
    "\n",
    "fold2 = df[features][116:231]\n",
    "y_fold2 = df['default'][116:231]\n",
    "\n",
    "fold3 = df[features][231:]\n",
    "y_fold3 = df['default'][231:]\n",
    "\n",
    "# reshaping for sklearn model\n",
    "X = fold1\n",
    "y = y_fold1\n",
    "X.head()\n",
    "\n",
    "# logistic regression model, ridge regression turned off\n",
    "logreg = linear_model.LogisticRegression(fit_intercept=False, C=1e9, solver='newton-cg')\n",
    "\n",
    "# coefficient of limit_rating variable\n",
    "logreg.fit(X, y).coef_[0][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient for the `limit_rating` variable is -0.001.  Now lets change turn Ridge Regression on by changing the parameter `C` to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4651660917476825e-05"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression model, ridge regression turned off\n",
    "logreg = linear_model.LogisticRegression(fit_intercept=False, C=1, solver='newton-cg')\n",
    "\n",
    "# coefficient of limit_rating variable\n",
    "logreg.fit(X, y).coef_[0][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient estimate for `limit_rating` is now 0.00001.  Ridge Regression has shrunk the parameter by approximately two orders of magnitude.  This shrinking of the coefficient reduces the effect that this variable has on hte model.  The sklearn implementation will use Ridge Regression by default, which is a nice feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore collinearity\n",
    "Up to this point we've seen how collinearity can make a model difficult to interpret.\n",
    "\n",
    "We've also looked at how to detect collinearity, and some of the potential sources of collinearity in your models.  \n",
    "\n",
    "Finally, we learned different techniques for dealing with collinearity.\n",
    "\n",
    "Now I'm going to tell you that collinearity doesn't matter, and you can just ignore it.  Well, sort of.  \n",
    "\n",
    "Depending on what your goals are, you might be able to [safely ignore collinearity](http://blog.minitab.com/blog/adventures-in-statistics-2/what-are-the-effects-of-multicollinearity-and-when-can-i-ignore-them).\n",
    "\n",
    "If the only goal of your logistic regression models is to make accurate predictions, then you can probably go ahead and ignore collinearity.  On the other hand, if your goal is to interpret your model and understand how your feature variables affect your outcome, then you can't ignore collinearity.  We went in-depth on this topic earlier, so I won't revisit that here.\n",
    "\n",
    "To show you that collinearity doesn't have an effect on the accuracy of your model's predictive power, lets run through an example.\n",
    "\n",
    "I'll be using the same `credit` dataset that we've been working with all along.  First I'll fit a model with the `income`, `limit`, and `rating` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79130434782608694"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Reading in the data\n",
    "df = pd.read_csv(\"default.csv\")\n",
    "\n",
    "# adding a column of ones to fit the intercept\n",
    "df['intercept'] = 1.0\n",
    "\n",
    "# feaure variables\n",
    "features = ['intercept','income','limit','rating']\n",
    "\n",
    "# creating 3 folds\n",
    "fold1 = df[features][0:116]\n",
    "y_fold1 = df['default'][0:116]\n",
    "\n",
    "fold2 = df[features][116:231]\n",
    "y_fold2 = df['default'][116:231]\n",
    "\n",
    "fold3 = df[features][231:]\n",
    "y_fold3 = df['default'][231:]\n",
    "\n",
    "\n",
    "# training data\n",
    "X = fold1\n",
    "y = y_fold1\n",
    "X.head()\n",
    "\n",
    "# test data\n",
    "X_test = fold2\n",
    "y_test = y_fold2\n",
    "\n",
    "# logistic regression model, ridge regression turned off\n",
    "logreg = linear_model.LogisticRegression(fit_intercept=False, C=1e9, solver='newton-cg')\n",
    "\n",
    "# coefficient of limit_rating variable\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# making predictions \n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# accuracy score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model is 79%.  We saw previously that `limit` and `rating` have some mild collinearity.  Lets remove the rating feature and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77391304347826084"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feaure variables\n",
    "features = ['intercept','income','limit']\n",
    "\n",
    "# creating 3 folds\n",
    "fold1 = df[features][0:116]\n",
    "y_fold1 = df['default'][0:116]\n",
    "\n",
    "fold2 = df[features][116:231]\n",
    "y_fold2 = df['default'][116:231]\n",
    "\n",
    "fold3 = df[features][231:]\n",
    "y_fold3 = df['default'][231:]\n",
    "\n",
    "\n",
    "# training data\n",
    "X = fold1\n",
    "y = y_fold1\n",
    "X.head()\n",
    "\n",
    "# test data\n",
    "X_test = fold2\n",
    "y_test = y_fold2\n",
    "\n",
    "# logistic regression model, ridge regression turned off\n",
    "logreg = linear_model.LogisticRegression(fit_intercept=False, C=1e9, solver='newton-cg')\n",
    "\n",
    "# coefficient of limit_rating variable\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# making predictions \n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# accuracy score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy has now changed slightly to 77%.  Removing the collinear variable had virtually no effect on our model.  We can also test this out by removing the `limit` feature and including the other collinear variable `rating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77391304347826084"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feaure variables\n",
    "features = ['intercept','income','rating']\n",
    "\n",
    "# creating 3 folds\n",
    "fold1 = df[features][0:116]\n",
    "y_fold1 = df['default'][0:116]\n",
    "\n",
    "fold2 = df[features][116:231]\n",
    "y_fold2 = df['default'][116:231]\n",
    "\n",
    "fold3 = df[features][231:]\n",
    "y_fold3 = df['default'][231:]\n",
    "\n",
    "\n",
    "# training data\n",
    "X = fold1\n",
    "y = y_fold1\n",
    "X.head()\n",
    "\n",
    "# test data\n",
    "X_test = fold2\n",
    "y_test = y_fold2\n",
    "\n",
    "# logistic regression model, ridge regression turned off\n",
    "logreg = linear_model.LogisticRegression(fit_intercept=False, C=1e9, solver='newton-cg')\n",
    "\n",
    "# coefficient of limit_rating variable\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# making predictions \n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# accuracy score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the accuracy is still 77%.\n",
    "\n",
    "These results illustrate an important point.  The existence of collinearity will have no effect on the outcome of the model.  If your goal is purely one of making accurate predictions, then its probably safe to ignore collinearity.  This is more of a machine learning perspective.  \n",
    "\n",
    "On the other hand, if your goal is to interpret your model, you shouldn't ignore collinearity.  Use the methods from this article to identify and deal with collinearity, and you'll be able to better interpret your results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
